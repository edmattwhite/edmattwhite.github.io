<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Ed's Blog</title><link>http://edmattwhite.github.io/</link><description>Recent content in Home on Ed's Blog</description><generator>Hugo</generator><language>en-gb</language><lastBuildDate>Tue, 29 Mar 2022 15:51:29 +0100</lastBuildDate><atom:link href="http://edmattwhite.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>What is Variational Inference?</title><link>http://edmattwhite.github.io/categories/stats/what-is-vi/</link><pubDate>Tue, 29 Mar 2022 15:51:29 +0100</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-vi/</guid><description>&lt;p&gt;Lots of problems have the property of being harder to solve in one direction than another – just imagine being asked to find the square root of a random integer compared with being asked to square one. We can also think about integration and differentiation as being a forward/inverse pair with an imbalance in their evaluation. In general, constructing a derivative can be done very easily using just a handful of simple rules. Integration, on the other hand, is much harder, and you may not even be able to find a closed form solution in what looks like a relatively simple problem.&lt;/p&gt;</description></item><item><title>What is a Generalised Linear Model?</title><link>http://edmattwhite.github.io/categories/stats/what-is-a-glm/</link><pubDate>Mon, 08 Nov 2021 21:38:24 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-a-glm/</guid><description>&lt;p&gt;Generalised Linear Models (GLMs) are a natural extension of simple normal linear regression to scenarios involving different kinds of responses and different assumptions.&lt;/p&gt;
&lt;h2 id="what-is-being-extended"&gt;What is being extended?&lt;/h2&gt;
&lt;p&gt;Linear Regression (or more precisely, the Normal Linear Model) is a very elegant tool for predicting real valued responses given some covariates, or features, that you believe have some ability to explain the trends that you see in your response. The actual model for your outcomes &lt;span class="math inline"&gt;\(Y \in \mathbb{R}^n\)&lt;/span&gt; given features &lt;span class="math inline"&gt;\(X \in \mathbb{R}^{n \times p}\)&lt;/span&gt; can be concisely expressed as &lt;span class="math display"&gt;\[\begin{equation}
Y \sim \text{Normal}\left(X\beta, \sigma^2 I_n \right),
\end{equation}\]&lt;/span&gt; where &lt;span class="math inline"&gt;\(I_n\)&lt;/span&gt; is the &lt;span class="math inline"&gt;\(n\)&lt;/span&gt;-dimensional identity matrix, &lt;span class="math inline"&gt;\(\sigma^2\)&lt;/span&gt; is some unknown variance parameter, and &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; is an unknown parameter defining how your data relates to the expectation of &lt;span class="math inline"&gt;\(Y\)&lt;/span&gt; (hint: &lt;em&gt;linearly&lt;/em&gt;).&lt;/p&gt;</description></item><item><title>What is a p-value?</title><link>http://edmattwhite.github.io/categories/stats/what-is-a-p-value/</link><pubDate>Wed, 03 Nov 2021 20:27:58 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-a-p-value/</guid><description>&lt;p&gt;&lt;span class="math inline"&gt;\(p\)&lt;/span&gt;-values need to be defined carefully to avoid miscommunication, but if you do understand them they’re a neat and powerful summarisation.&lt;/p&gt;
&lt;h2 id="why-do-we-talk-about-p-values"&gt;Why do we talk about &lt;span class="math inline"&gt;\(p\)&lt;/span&gt;-values?&lt;/h2&gt;
&lt;p&gt;The scientific method is built around the idea of formulating and testing hypotheses. Statistics can give us a way of &lt;em&gt;testing&lt;/em&gt; or &lt;em&gt;comparing&lt;/em&gt; hypotheses &lt;a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Historically, frequentist statistics has been the framework that these tests have been constructed under. This involves the assumption that our data are random, and that they are generated by some underlying process which has fixed parameters. Test statistics (such as &lt;span class="math inline"&gt;\(z\)&lt;/span&gt;-scores and &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;-scores) are calculated from our data and are useful exactly because we know how they are distributed under our hypothesis.&lt;/p&gt;</description></item><item><title>GMM -- An EM Example</title><link>http://edmattwhite.github.io/categories/stats/how-to-em-algorithm/</link><pubDate>Tue, 23 Feb 2021 21:53:28 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/how-to-em-algorithm/</guid><description>&lt;h2 id="gaussian-mixture-model"&gt;Gaussian Mixture Model&lt;/h2&gt;
&lt;p&gt;Mixture models are one of the most simple examples of a latent variable model (by which I mean a model where some random variable, commonly refered to as &lt;span class="math inline"&gt;\(Z\)&lt;/span&gt;, is not observable). A &lt;em&gt;Gaussian Mixture Model&lt;/em&gt; (GMM) is one where our quantity of interest can be considered to follow a Normal distribution (which can be multivariate). There’s obviously something more going on – the mixture part comes from the fact that the parameters for this Normal distribution aren’t fixed for every observation we might make – they are in fact drawn from a finite set.&lt;/p&gt;</description></item><item><title>What is the Expectation Maximisation algorithm?</title><link>http://edmattwhite.github.io/categories/stats/what-is-em-algorithm/</link><pubDate>Mon, 22 Feb 2021 20:33:41 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-em-algorithm/</guid><description>&lt;p&gt;Powerful statistical models are often useful because they are very expressive. The structure of the model can lend itself well to describing a real world process. Unfortunately, real world processes are incredibly rude and often lead to intractable likelihoods. The Expectation Maximisation (EM) algorithm offers us a way around this by giving an iterative procedure for find MAP (or maximum likelihood) estimates for parameters.&lt;/p&gt;
&lt;p&gt;One thing to consider when reading this is that the EM algorithm is less an algorithm in itself, but more a method to find algorithms.&lt;/p&gt;</description></item><item><title>About</title><link>http://edmattwhite.github.io/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://edmattwhite.github.io/about/</guid><description>&lt;p&gt;I&amp;rsquo;m currently an MSc. student at Imperial College London &amp;ndash; this is my personal website, which is where I put things that I find interesting when I get the time to write about them.
Mostly these are explanations of things that I found tricky to begin with and wished for an explanation written by myself in the future once I did understand them!&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m interested in maths (at the moment mostly statistics/probability and computational methods around them), music and rowing.&lt;/p&gt;</description></item></channel></rss>