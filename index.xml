<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Ed's Blog</title><link>http://edmattwhite.github.io/</link><description>Recent content in Home on Ed's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Mon, 08 Nov 2021 21:38:24 +0000</lastBuildDate><atom:link href="http://edmattwhite.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>What is a Generalised Linear Model?</title><link>http://edmattwhite.github.io/categories/stats/what-is-a-glm/</link><pubDate>Mon, 08 Nov 2021 21:38:24 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-a-glm/</guid><description>Generalised Linear Models (GLMs) are a natural extension of simple normal linear regression to scenarios involving different kinds of responses and different assumptions.
What is being extended? Linear Regression (or more precisely, the Normal Linear Model) is a very elegant tool for predicting real valued responses given some covariates, or features, that you believe have some ability to explain the trends that you see in your response. The actual model for your outcomes \(Y \in \mathbb{R}^n\) given features \(X \in \mathbb{R}^{n \times p}\) can be concisely expressed as \[\begin{equation} Y \sim \text{Normal}\left(X\beta, \sigma^2 I_n \right), \end{equation}\] where \(I_n\) is the \(n\)-dimensional identity matrix, \(\sigma^2\) is some unknown variance parameter, and \(\beta\) is an unknown parameter defining how your data relates to the expectation of \(Y\) (hint: linearly).</description></item><item><title>What is a p-value?</title><link>http://edmattwhite.github.io/categories/stats/what-is-a-p-value/</link><pubDate>Wed, 03 Nov 2021 20:27:58 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-a-p-value/</guid><description>\(p\)-values need to be defined carefully to avoid miscommunication, but if you do understand them they’re a neat and powerful summarisation.
Why do we talk about \(p\)-values? The scientific method is built around the idea of formulating and testing hypotheses. Statistics can give us a way of testing or comparing hypotheses 1.
Historically, frequentist statistics has been the framework that these tests have been constructed under. This involves the assumption that our data are random, and that they are generated by some underlying process which has fixed parameters.</description></item><item><title>GMM -- An EM Example</title><link>http://edmattwhite.github.io/categories/stats/how-to-em-algorithm/</link><pubDate>Tue, 23 Feb 2021 21:53:28 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/how-to-em-algorithm/</guid><description>Gaussian Mixture Model Mixture models are one of the most simple examples of a latent variable model (by which I mean a model where some random variable, commonly refered to as \(Z\), is not observable). A Gaussian Mixture Model (GMM) is one where our quantity of interest can be considered to follow a Normal distribution (which can be multivariate). There’s obviously something more going on – the mixture part comes from the fact that the parameters for this Normal distribution aren’t fixed for every observation we might make – they are in fact drawn from a finite set.</description></item><item><title>What is the Expectation Maximisation algorithm?</title><link>http://edmattwhite.github.io/categories/stats/what-is-em-algorithm/</link><pubDate>Mon, 22 Feb 2021 20:33:41 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-em-algorithm/</guid><description>Powerful statistical models are often useful because they are very expressive. The structure of the model can lend itself well to describing a real world process. Unfortunately, real world processes are incredibly rude and often lead to intractable likelihoods. The Expectation Maximisation (EM) algorithm offers us a way around this by giving an iterative procedure for find MAP (or maximum likelihood) estimates for parameters.
One thing to consider when reading this is that the EM algorithm is less an algorithm in itself, but more a method to find algorithms.</description></item><item><title>About</title><link>http://edmattwhite.github.io/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://edmattwhite.github.io/about/</guid><description>I&amp;rsquo;m currently an MSc. student at Imperial College London &amp;ndash; this is my personal website, which is where I put things that I find interesting when I get the time to write about them. Mostly these are explanations of things that I found tricky to begin with and wished for an explanation written by myself in the future once I did understand them!
I&amp;rsquo;m interested in maths (at the moment mostly statistics/probability and computational methods around them), music and rowing.</description></item></channel></rss>