<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet type=text/css href=/css/style.css><link rel=icon href=/images/favicon.ico type=image/x-icon><title>Ed's Blog | GMM -- An EM Example</title></head><body><nav><img src=/images/logo.svg alt="Ed's Blog"><li><a href=/>Home</a></li><li><a href=/about/>About</a></li><li><a href=/categories/>Categories</a></li></nav><div class=content><div class=article-meta><h1><span class=title>GMM – An EM Example</span></h1><h2 class=date>23/02/2021</h2></div><main><h2 id=gaussian-mixture-model>Gaussian Mixture Model</h2><p>Mixture models are one of the most simple examples of a latent variable model (by which I mean a model where some random variable, commonly refered to as <span class="math inline">\(Z\)</span>, is not observable). A <em>Gaussian Mixture Model</em> (GMM) is one where our quantity of interest can be considered to follow a Normal distribution (which can be multivariate). There’s obviously something more going on – the mixture part comes from the fact that the parameters for this Normal distribution aren’t fixed for every observation we might make – they are in fact drawn from a finite set.</p><p>More formally, we say that <span class="math inline">\(X \sim GMM(p, \{ \mu_k \}, \{ \Sigma_k \})\)</span> if:</p><p><span class="math display">\[\begin{align}
Z &\sim \text{Categorical}(p) \\
X &\sim \text{Normal}(\mu_Z, \Sigma_Z)
\end{align}\]</span></p><p>The intermediate variable <span class="math inline">\(Z\)</span> allows us to build multimodal distributions (something that would be very useful) from unimodal Normal distributions (something that is very well studied). The only issue comes when we want to actually fit the thing.</p><p>Putting aside priors on parameters for the moment, let’s assume that we want maximum likelihood estimates for our parameters <span class="math inline">\(p, \mu_k, \Sigma_k\)</span>. (We can do this by setting our prior term in our <span class="math inline">\(Q\)</span> function to a constant, and then realising that we can then just drop it for the purposes of maximisation.)</p><h2 id=em-gmm>EM + GMM</h2><p>So we take a look at the form of the likelihood for a collection of data <span class="math inline">\(\mathbf{X} = \left\{X_i : i \in \{1, \dots, n \}\right\}\)</span>, with associated latent variables <span class="math inline">\(\mathbf{Z} = \left\{Z_i : i \in \{1, \dots, n \}\right\}\)</span> and <span class="math inline">\(\theta = \left\{ p_{ij}, \mu_j, \Sigma_j : i \in \{1, \dots, n\}, j \in \{ 1, \dots, k\}\right\}\)</span>:</p><p><span class="math display">\[\begin{align}
f( \mathbf{X} \vert \theta, \mathbf{Z} ) &= \prod_i f_{\mathcal{N}}(x \vert \mu_{Z_i}, \Sigma_{Z_i}) \\
f( \mathbf{Z} \vert \theta) &= \prod_i p_{iZ_i}
\end{align}\]</span></p><p>We then obtain our <span class="math inline">\(Q\)</span> function by taking the expectation of the sum of the logs of these two densities, where the expectation is with respect to <span class="math inline">\(Z_i \vert \varphi, X_i\)</span> for each <span class="math inline">\(i\)</span>. This is one of the most crucial steps of the algorithm, and it’s really important to be clear about exactly what this distribution is. In this case, as <span class="math inline">\(Z\)</span> can only take a finite number of values we have a simple expression for this from Bayes’ rule:</p><p><span class="math display">\[\begin{align}
\mathbb{P}( Z = j \vert X, \varphi)
&=
\frac{ \mathbb{P}( Z = j, X, \varphi) }{ \mathbb{P}( X, \varphi) }\\
&=
\frac{ \mathbb{P}( X \vert Z =j, \varphi) \mathbb{P}( Z = j \vert \varphi) }
{ \sum_{m} \mathbb{P}( X \vert Z = m, \varphi) \mathbb{P}( Z = m \vert \varphi) }\\
\end{align}\]</span></p><p>We’re going to call these reweightings (which are <em>not</em> the same as the <span class="math inline">\(p^{\varphi}_{ij})\)</span> <span class="math inline">\(q^{\varphi}_{ij}\)</span>. The above densities are simply normal and categorical from the definition of the model.</p><p><span class="math display">\[\begin{align}
Q(\theta, \varphi)
&=
\mathbb{E}_{\mathbf{Z} \sim Z \vert \varphi, X}
\left[
\sum_i
- \frac{1}{2} \log \left( (2 \pi)^d \det\left(\Sigma^\theta_{Z_i}\right) \right)
- \frac{1}{2} (x_i - \mu^\theta_{Z_i})^T \Sigma^{{-1}^{\theta}}_{Z_i} (x_i - \mu^\theta_{Z_i})
+ \log p^\theta_{iZ_i}
\right]
\\
&=
\sum_i
\sum_j
- q^\varphi_{ij} \frac{1}{2} \log \left( (2 \pi)^d \det\left(\Sigma^\theta_j\right) \right)
- q^\varphi_{ij} \frac{1}{2} (x_i - \mu^\theta_j)^T \Sigma^{{-1}^\theta}_j (x_i - \mu^\theta_j)
+ q^\varphi_{ij} \log p^\theta_{ij}
\end{align}\]</span></p><p>I’ve add superscripts to each of the variables so that we can see which parameter estimate <span class="math inline">\(\theta\)</span> or <span class="math inline">\(\varphi\)</span> it is associated with. The task is now to maximise <span class="math inline">\(Q\)</span> with respect to the <span class="math inline">\(\theta\)</span> parameters, which happily we can do analytically<a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a> (using <span class="math inline">\(\hat\cdot\)</span> to indicate the maximum).</p><p><span class="math display">\[\begin{align}
\hat{\mu}^\theta_j &= \frac{\sum_i p^\varphi_{ij} x_i}{\sum_i p^\varphi_{ij}} \\
\hat{\Sigma}^\theta_j &= \frac{\sum_i p^\varphi_{ij} ( x_i - \hat{\mu}^\theta_j ) (x_i - \hat{\mu}^\theta_j)^T }{ \sum_i p^\varphi_{ij} } \\
\hat{p}^{\theta}_{ij} &= q^{\varphi}_{ij}
\end{align}\]</span></p><p>Now at each iteration of our algorithm, we can use the parameters of the current step to generate <em>better</em> parameters (in the sense of maximising the likelihood) according to the update step above – if we say <span class="math inline">\(\varphi = \theta_t\)</span>, then <span class="math inline">\(\theta_{t+1} = \hat{\theta}\)</span> in the above relations.</p><p>There is a neat interpretation to this update step where we’re performing a sort of weighted maximum likelihood estimate for the parameters of the normal distribution based on the cluster assignments of the previous step, and then updating our cluster assignments for the new step based on the old weighted cluster assignment – there’s a sort of lock step where the two sets of parameters are constantly playing catch up with each other to best fit the data.</p><h2 id=what-else>What else?</h2><p>This example covered a classic “latent variable” type use of the EM algorithm, but it also works really well in situations where you have partially or unobserved data – in this case the <span class="math inline">\(Z\)</span> values become a sort of obscured <span class="math inline">\(X\)</span> value, and can be really useful in cases where your measurement of a system imposes some kind of censoring on the values.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn1 role=doc-endnote><p>We find the maximum of this using exactly the same techniques that we exploit to find maximum likelihood estimates – namely differentiation, log-concavity and the use of a Lagrange multiplier to enforce the sum-to-one constraint on the <span class="math inline">\(p_{ij}\)</span>.<a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></main></div><footer><p>&copy; 2022 - Edward White</p></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src=/js/mathjax-config.js async></script></script></body></html>