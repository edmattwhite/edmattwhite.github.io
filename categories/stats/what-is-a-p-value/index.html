<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet type=text/css href=/css/style.css><link rel=icon href=/images/favicon.ico type=image/x-icon><title>Ed's Blog | What is a p-value?</title></head><body><nav><img src=/images/logo.svg alt="Ed's Blog"><li><a href=/>Home</a></li><li><a href=/about/>About</a></li><li><a href=/categories/>Categories</a></li></nav><div class=content><div class=article-meta><h1><span class=title>What is a p-value?</span></h1><h2 class=date>03/11/2021</h2></div><main><p><span class="math inline">\(p\)</span>-values need to be defined carefully to avoid miscommunication, but if you do understand them they’re a neat and powerful summarisation.</p><h2 id=why-do-we-talk-about-p-values>Why do we talk about <span class="math inline">\(p\)</span>-values?</h2><p>The scientific method is built around the idea of formulating and testing hypotheses. Statistics can give us a way of <em>testing</em> or <em>comparing</em> hypotheses <a href=#fn1 class=footnote-ref id=fnref1 role=doc-noteref><sup>1</sup></a>.</p><p>Historically, frequentist statistics has been the framework that these tests have been constructed under. This involves the assumption that our data are random, and that they are generated by some underlying process which has fixed parameters. Test statistics (such as <span class="math inline">\(z\)</span>-scores and <span class="math inline">\(t\)</span>-scores) are calculated from our data and are useful exactly because we know how they are distributed under our hypothesis.</p><p>What is distinctly less useful is that different test statistics are very hard to compare – if we see <span class="math inline">\(t\)</span>-scores published then how can we compare them with a different test statistic, like that from a Kolmogorov-Smirnov test?</p><p>The <span class="math inline">\(p\)</span>-value attempts to solve this problem and make different tests comparable in a meaningful way. Unfortunately, because of the layers of abstraction that are built upon and the way that people like to think about probability statements, it’s often misunderstood.</p><h2 id=how-do-we-get-there>How do we get there?</h2><p>Let’s step back for a second and consider what we’re doing. In our experiment we’re taking a sample <span class="math inline">\(X\)</span> from some population, and we’re trying to learn something about the parameters <span class="math inline">\(\theta\)</span> that govern the population from that sample.</p><p>We have decided that the best way to do this is by formulating an idea and testing to see whether it’s true or not.<a href=#fn2 class=footnote-ref id=fnref2 role=doc-noteref><sup>2</sup></a></p><blockquote><p>A hypothesis <span class="math inline">\(H\)</span> is a statement about our belief in possible values of the population parameter: <span class="math inline">\(\theta \in \Theta\)</span>.</p></blockquote><blockquote><p>A hypothesis test comparing two hypotheses <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> is a procedure that specifies: 1. For which sample values the decision is made to accept <span class="math inline">\(H_0\)</span>. 1. For which sample values <span class="math inline">\(H_0\)</span> is rejected and <span class="math inline">\(H_1\)</span> is accepted as true.</p></blockquote><p>Given that our <span class="math inline">\(X\)</span> are randomly generated, even if we had perfect knowledge of <span class="math inline">\(\theta\)</span> then we won’t know the exact value for <span class="math inline">\(X\)</span>. This also carries across to any test statistic that we might construct from <span class="math inline">\(X\)</span>. To try and make this easier to deal with, a <span class="math inline">\(p\)</span>-value combines the idea of a test statistic with that test statistic’s observation probability under <span class="math inline">\(H_0\)</span>:</p><blockquote><p>A <span class="math inline">\(p\)</span>-value <span class="math inline">\(p(X)\)</span> is a test statistic satisfying <span class="math inline">\(p(X) \in [0, 1]\)</span> for every sample point <span class="math inline">\(X\)</span>. A valid <span class="math inline">\(p\)</span>-value has that for every <span class="math inline">\(\theta \in \Theta_0\)</span>, and every <span class="math inline">\(\gamma \in [0, 1]\)</span>: <span class="math display">\[ \mathbb{P}\left(p(X) \leq \gamma \vert \theta \right) \leq \gamma \]</span></p></blockquote><p>Importantly, note that our definition of the <span class="math inline">\(p\)</span>-value doesn’t make any kind of probabilistic statement about the veracity of <span class="math inline">\(H_0\)</span>.</p><p><strong>A <span class="math inline">\(p\)</span>-value simply states the probability of observing data at least as `unusual’ as the sample given <span class="math inline">\(H_0\)</span>.</strong></p><h2 id=why-are-they-useful>Why are they useful?</h2><p>If we’re performing a test, then regardless of the underlying distributional assumptions of that test, we should be able to find a <span class="math inline">\(p\)</span>-value to communicate the result of testing a particular sample. This gives us a way to more easily compare different testing procedures <a href=#fn3 class=footnote-ref id=fnref3 role=doc-noteref><sup>3</sup></a>, which was our primary goal. One more thing that the <span class="math inline">\(p\)</span>-value gives us is the ability to set our own personal threshold for what we believe an extreme value for a test statistic should be. Results in particle physics are often considered significant only once they’ve reached the ``5-sigma’’ level (which is a <span class="math inline">\(p\)</span>-value <span class="math inline">\(&lt; 3.5 \times 10 ^ {-7}\)</span>), whilst social scientists often talk about <span class="math inline">\(p\)</span>-values <span class="math inline">\(&lt; 0.05\)</span> as being of interest. The <span class="math inline">\(p\)</span>-value allows us to draw a line under which we’re happy to start talking about an effect as being statistically significant, and ensures that line is able to be compared across different experiments.</p><h2 id=whats-the-problem>What’s the problem?</h2><p>Apart from the fact that people are really bad at understanding what <span class="math inline">\(p\)</span>-values actually mean, there’s a bigger problem here. Let’s say that I start accepting any scientific study that has observed an effect (or rejected <span class="math inline">\(H_0\)</span>) with a <span class="math inline">\(p\)</span>-value less than some threshold (say 0.05). Then, by the definition of the <span class="math inline">\(p\)</span>-value, if I were to take a collection of 20 findings that I believed were true, I would expect that in fact one of them was false.</p><p>By itself this is not awful – if 95% of published findings were correct then that would be great! However, the problem comes when publishing is made conditional on a <span class="math inline">\(p\)</span>-value threshold, as in that case a large enough community interested in a phenomenon will be certain to generate an experimental result with the requisite <span class="math inline">\(p\)</span>-value after some time. This can’t be put in context because anyone performing an experiment and not finding the evidence to support a new hypothesis won’t have been incentivised to make that `non-discovery’ widely known. In this world, none of our scientists have done anything wrong, but an over reliance on the <span class="math inline">\(p\)</span>-value as a threshold has led to an undesirable result, and people misunderstanding <span class="math inline">\(p\)</span>-values (as some probability of <span class="math inline">\(H_0\)</span> being true) means that what is published is given the wrong interpretation!</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn1 role=doc-endnote><p>Casella G, Berger RL. Statistical inference. Cengage Learning; 2021 Jan 26.<a href=#fnref1 class=footnote-back role=doc-backlink>↩︎</a></p></li><li id=fn2 role=doc-endnote><p>It’s worth noting that we could have done something else – for example run a parameter estimation procedure – hypothesis testing is not the beginning and end of statistical inference!<a href=#fnref2 class=footnote-back role=doc-backlink>↩︎</a></p></li><li id=fn3 role=doc-endnote><p>However we do still need to be aware of the procedures and assumptions behind different tests.<a href=#fnref3 class=footnote-back role=doc-backlink>↩︎</a></p></li></ol></section></main></div><footer><p>&copy; 2026 - Edward White</p></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src=/js/mathjax-config.js async></script></script></body></html>