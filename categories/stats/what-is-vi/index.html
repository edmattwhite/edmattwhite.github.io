<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet type=text/css href=/css/style.css><link rel=icon href=/images/favicon.ico type=image/x-icon><title>Ed's Blog | What is Variational Inference?</title></head><body><nav><img src=/images/logo.svg alt="Ed's Blog"><li><a href=/>Home</a></li><li><a href=/about/>About</a></li><li><a href=/categories/>Categories</a></li></nav><div class=content><div class=article-meta><h1><span class=title>What is Variational Inference?</span></h1><h2 class=date>29/03/2022</h2></div><main><p>Lots of problems have the property of being harder to solve in one direction than another – just imagine being asked to find the square root of a random integer compared with being asked to square one. We can also think about integration and differentiation as being a forward/inverse pair with an imbalance in their evaluation. In general, constructing a derivative can be done very easily using just a handful of simple rules. Integration, on the other hand, is much harder, and you may not even be able to find a closed form solution in what looks like a relatively simple problem.</p><p>This has a bearing on how we talk about probabilistic modelling – marginalisation is an integration operation, and is therefore difficult. Bayesian analysis has to grapple with this problem of being presented with integrals that are generally speaking intractable. Suppose that we have a latent variable <span class="math inline">\(Z\)</span> that we want to calculate the posterior for given data <span class="math inline">\(X\)</span>, and that we have densities <span class="math inline">\(p\)</span>, then we see:</p><p><span class="math display">\[\begin{equation}
p(z \vert x)
= \frac{p(x | z)p(z)}{p(x)}
= \frac{p(x | z)p(z)}{\int p(x | z')p(z') dz'}.
\end{equation}\]</span></p><p>Our posterior calculation involves an integral over the entirety of the domain of <span class="math inline">\(Z\)</span> – which even for fairly low dimensional problems can be infeasible. Note that this is <em>similar</em> to the <a href=http://edmattwhite.github.io/categories/stats/what-is-em-algorithm/>EM algorithm</a>, in that our initial approach has been blocked by a difficult integral.</p><h2 id=variational-inference-to-the-rescue>Variational Inference to the Rescue</h2><p>At its core, variational inference is a neat trick for turning a hard problem (integration) into an easy one (differentiation). This trade has to come at a cost – we move away from the world of exact inference, and accept that we will approximate a distribution of interest.</p><p>We do this by introducing a ‘variational family’ of distributions that we allow our approximation to be drawn from – in practice this can be a reasonable assumption, but care has to be taken in situations where these approximations are fed in to other machinery. The family that we use to approximate this distribution, the ‘variational family’ will have the feature that we can index it with some parameter <span class="math inline">\(\varphi\)</span>, so that changing <span class="math inline">\(\varphi\)</span> allows us to smoothly traverse the entire set of distributions in that family. The associated density is usually denoted <span class="math inline">\(q_{\varphi}\)</span> – note that we’ve dropped the <span class="math inline">\(x\)</span> from the dependence on <span class="math inline">\(x\)</span> in the argument in the density, as in our approximation dependence on <span class="math inline">\(X\)</span> passes through <span class="math inline">\(\varphi\)</span>.</p><p>If we want to now find an optimal <span class="math inline">\(\varphi\)</span> we need to find some useful objective. We know the model evidence <span class="math inline">\(p(x)\)</span> is fixed, so we proceed by suggestively playing around with that quantity:</p><p><span class="math display">\[\begin{align}
\log p(x)
&= \log p(x) + \log p(z \vert x) - \log p(z \vert x)\\
&= \log p(z, x) - \log p(z \vert x)\\
&= \log p(z, x) - \log q_\varphi(z) + \log q_\varphi(z) - \log p(z \vert x)
\end{align}\]</span></p><p>All we’ve done so far is throw in a load of quantities that are related to things that we’re interested in. The next step just involves taking the expectation with respect to the variational distribution <span class="math inline">\(q_\varphi\)</span>, and using the definition of KL divergence to get:</p><p><span class="math display">\[\begin{align}
\log p(x)
&= \mathbb{E}_{q_\varphi}\left[\log p(z, x) - \log q_\varphi(z) + \log q_\varphi(z) - \log p(z \vert x) \right] \\
&= \mathbb{E}_{q_\varphi}\left[\log p(z, x) - \log q_\varphi(z) \right] + D_{KL}\left( q_\varphi \Vert p(z \vert x) \right)
\end{align}\]</span></p><p>This is now a familiar story if you’ve seen the EM algorithm’s derivation – the left hand side is fixed, so we know that increasing one quantity on the right hand side will decrease the other and vice versa. The strange quantity <span class="math inline">\(\mathbb{E}_{q_\varphi}\left[\log p(z, x) - \log q_\varphi(z) \right]\)</span> is called the ELBO, `short’ for Evidence Lower BOund. Maximising this quantity will minimise the divergence between our approximation of the posterior and the true posterior, and that is all that variational inference (in its various forms) does.</p></main></div><footer><p>&copy; 2022 - Edward White</p></footer><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src=/js/mathjax-config.js async></script></script></body></html>