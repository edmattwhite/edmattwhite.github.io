<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Categories on Ed's Blog</title><link>http://edmattwhite.github.io/categories/</link><description>Recent content in Categories on Ed's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Wed, 03 Nov 2021 20:27:58 +0000</lastBuildDate><atom:link href="http://edmattwhite.github.io/categories/index.xml" rel="self" type="application/rss+xml"/><item><title>What is a p-value?</title><link>http://edmattwhite.github.io/categories/stats/what-is-a-p-value/</link><pubDate>Wed, 03 Nov 2021 20:27:58 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-a-p-value/</guid><description>\(p\)-values need to be defined carefully to avoid miscommunication, but if you do understand them they’re a neat and powerful summarisation.
Why do we talk about \(p\)-values? The scientific method is built around the idea of formulating and testing hypotheses. Statistics can give us a way of testing or comparing hypotheses 1.
Historically, frequentist statistics has been the framework that these tests have been constructed under. This involves the assumption that our data are random, and that they are generated by some underlying process which has fixed parameters.</description></item><item><title>GMM -- An EM Example</title><link>http://edmattwhite.github.io/categories/stats/how-to-em-algorithm/</link><pubDate>Tue, 23 Feb 2021 21:53:28 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/how-to-em-algorithm/</guid><description>Gaussian Mixture Model Mixture models are one of the most simple examples of a latent variable model (by which I mean a model where some random variable, commonly refered to as \(Z\), is not observable). A Gaussian Mixture Model (GMM) is one where our quantity of interest can be considered to follow a Normal distribution (which can be multivariate). There’s obviously something more going on – the mixture part comes from the fact that the parameters for this Normal distribution aren’t fixed for every observation we might make – they are in fact drawn from a finite set.</description></item><item><title>What is the Expectation Maximisation algorithm?</title><link>http://edmattwhite.github.io/categories/stats/what-is-em-algorithm/</link><pubDate>Mon, 22 Feb 2021 20:33:41 +0000</pubDate><guid>http://edmattwhite.github.io/categories/stats/what-is-em-algorithm/</guid><description>Powerful statistical models are often useful because they are very expressive. The structure of the model can lend itself well to describing a real world process. Unfortunately, real world processes are incredibly rude and often lead to intractable likelihoods. The Expectation Maximisation (EM) algorithm offers us a way around this by giving an iterative procedure for find MAP (or maximum likelihood) estimates for parameters.
One thing to consider when reading this is that the EM algorithm is less an algorithm in itself, but more a method to find algorithms.</description></item></channel></rss>